#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CUDA ç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºæ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ”¯æŒ CUDA ä»¥åŠç›¸å…³åº“çš„å¯ç”¨æ€§
"""

import sys
import platform
from typing import Dict, Any


def test_python_version() -> Dict[str, Any]:
    """æµ‹è¯• Python ç‰ˆæœ¬"""
    print("=" * 50)
    print("Python ç‰ˆæœ¬ä¿¡æ¯")
    print("=" * 50)
    
    version_info = {
        "python_version": sys.version,
        "platform": platform.platform(),
        "architecture": platform.architecture(),
        "machine": platform.machine()
    }
    
    print(f"Python ç‰ˆæœ¬: {version_info['python_version']}")
    print(f"å¹³å°: {version_info['platform']}")
    print(f"æ¶æ„: {version_info['architecture']}")
    print(f"æœºå™¨ç±»å‹: {version_info['machine']}")
    
    return version_info


def test_torch_cuda() -> Dict[str, Any]:
    """æµ‹è¯• PyTorch CUDA æ”¯æŒ"""
    print("\n" + "=" * 50)
    print("PyTorch CUDA æ”¯æŒæµ‹è¯•")
    print("=" * 50)
    
    try:
        import torch
        torch_info = {
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A",
            "cudnn_version": torch.backends.cudnn.version() if torch.cuda.is_available() else "N/A",
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        print(f"PyTorch ç‰ˆæœ¬: {torch_info['torch_version']}")
        print(f"CUDA å¯ç”¨: {torch_info['cuda_available']}")
        
        if torch_info['cuda_available']:
            print(f"CUDA ç‰ˆæœ¬: {torch_info['cuda_version']}")
            print(f"cuDNN ç‰ˆæœ¬: {torch_info['cudnn_version']}")
            print(f"GPU æ•°é‡: {torch_info['device_count']}")
            
            # æ˜¾ç¤ºæ¯ä¸ª GPU çš„ä¿¡æ¯
            for i in range(torch_info['device_count']):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
                
            # æµ‹è¯• CUDA å¼ é‡æ“ä½œ
            print("\næµ‹è¯• CUDA å¼ é‡æ“ä½œ...")
            device = torch.device("cuda:0")
            x = torch.randn(1000, 1000).to(device)
            y = torch.randn(1000, 1000).to(device)
            z = torch.mm(x, y)
            print("âœ“ CUDA å¼ é‡æ“ä½œæˆåŠŸ")
            
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            
        return torch_info
        
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return {"error": "PyTorch not installed"}


def test_transformers() -> Dict[str, Any]:
    """æµ‹è¯• Transformers åº“"""
    print("\n" + "=" * 50)
    print("Transformers åº“æµ‹è¯•")
    print("=" * 50)
    
    try:
        import transformers
        transformers_info = {
            "transformers_version": transformers.__version__,
            "tokenizers_version": getattr(transformers, '__tokenizers_version__', 'N/A')
        }
        
        print(f"Transformers ç‰ˆæœ¬: {transformers_info['transformers_version']}")
        print(f"Tokenizers ç‰ˆæœ¬: {transformers_info['tokenizers_version']}")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("\næµ‹è¯•æ¨¡å‹åŠ è½½...")
        from transformers import AutoTokenizer, AutoModelForCausalLM
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", trust_remote_code=True)
        print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")
        
        return transformers_info
        
    except ImportError:
        print("âŒ Transformers æœªå®‰è£…")
        return {"error": "Transformers not installed"}
    except Exception as e:
        print(f"âŒ Transformers æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}


def test_peft() -> Dict[str, Any]:
    """æµ‹è¯• PEFT åº“"""
    print("\n" + "=" * 50)
    print("PEFT åº“æµ‹è¯•")
    print("=" * 50)
    
    try:
        import peft
        peft_info = {
            "peft_version": peft.__version__
        }
        
        print(f"PEFT ç‰ˆæœ¬: {peft_info['peft_version']}")
        print("âœ“ PEFT åº“å¯ç”¨")
        
        return peft_info
        
    except ImportError:
        print("âŒ PEFT æœªå®‰è£…")
        return {"error": "PEFT not installed"}


def test_bitsandbytes() -> Dict[str, Any]:
    """æµ‹è¯• bitsandbytes åº“"""
    print("\n" + "=" * 50)
    print("bitsandbytes åº“æµ‹è¯•")
    print("=" * 50)
    
    try:
        import bitsandbytes as bnb
        bnb_info = {
            "bitsandbytes_version": bnb.__version__
        }
        
        print(f"bitsandbytes ç‰ˆæœ¬: {bnb_info['bitsandbytes_version']}")
        
        # å°è¯•ä¸åŒçš„æ–¹æ³•æ¥æ£€æŸ¥ CUDA æ”¯æŒ
        cuda_available = False
        try:
            # æ–¹æ³•1ï¼šå°è¯•å¯¼å…¥ CUDA ç›¸å…³æ¨¡å—
            from bitsandbytes.cuda_setup.main import get_compute_capability
            compute_cap = get_compute_capability()
            cuda_available = compute_cap is not None
            print(f"CUDA æ”¯æŒ: {cuda_available}")
            if cuda_available:
                print(f"è®¡ç®—èƒ½åŠ›: {compute_cap}")
        except ImportError:
            try:
                # æ–¹æ³•2ï¼šå°è¯•ç›´æ¥è®¿é—®
                compute_cap = bnb.cuda_setup.get_compute_capability()
                cuda_available = compute_cap is not None
                print(f"CUDA æ”¯æŒ: {cuda_available}")
                if cuda_available:
                    print(f"è®¡ç®—èƒ½åŠ›: {compute_cap}")
            except AttributeError:
                try:
                    # æ–¹æ³•3ï¼šå°è¯•å…¶ä»–å±æ€§
                    cuda_available = hasattr(bnb, 'cuda_setup')
                    print(f"CUDA æ”¯æŒ: {cuda_available}")
                except:
                    print("æ— æ³•ç¡®å®š CUDA æ”¯æŒçŠ¶æ€")
        
        bnb_info["cuda_available"] = cuda_available
        
        if cuda_available:
            print("âœ“ bitsandbytes CUDA æ”¯æŒå¯ç”¨")
        else:
            print("âš  bitsandbytes æ²¡æœ‰ CUDA æ”¯æŒ")
            
        return bnb_info
        
    except ImportError:
        print("âŒ bitsandbytes æœªå®‰è£…")
        return {"error": "bitsandbytes not installed"}


def test_accelerate() -> Dict[str, Any]:
    """æµ‹è¯• Accelerate åº“"""
    print("\n" + "=" * 50)
    print("Accelerate åº“æµ‹è¯•")
    print("=" * 50)
    
    try:
        import accelerate
        accelerate_info = {
            "accelerate_version": accelerate.__version__
        }
        
        print(f"Accelerate ç‰ˆæœ¬: {accelerate_info['accelerate_version']}")
        
        # æµ‹è¯• Accelerate é…ç½®
        from accelerate import Accelerator
        accelerator = Accelerator()
        print("âœ“ Accelerate åˆå§‹åŒ–æˆåŠŸ")
        
        return accelerate_info
        
    except ImportError:
        print("âŒ Accelerate æœªå®‰è£…")
        return {"error": "Accelerate not installed"}


def test_datasets() -> Dict[str, Any]:
    """æµ‹è¯• Datasets åº“"""
    print("\n" + "=" * 50)
    print("Datasets åº“æµ‹è¯•")
    print("=" * 50)
    
    try:
        import datasets
        datasets_info = {
            "datasets_version": datasets.__version__
        }
        
        print(f"Datasets ç‰ˆæœ¬: {datasets_info['datasets_version']}")
        print("âœ“ Datasets åº“å¯ç”¨")
        
        return datasets_info
        
    except ImportError:
        print("âŒ Datasets æœªå®‰è£…")
        return {"error": "Datasets not installed"}


def generate_summary(all_results: Dict[str, Any]):
    """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶
    torch_ok = all_results.get('torch', {}).get('cuda_available', False)
    transformers_ok = 'error' not in all_results.get('transformers', {})
    peft_ok = 'error' not in all_results.get('peft', {})
    datasets_ok = 'error' not in all_results.get('datasets', {})
    
    print(f"PyTorch CUDA æ”¯æŒ: {'âœ“' if torch_ok else 'âŒ'}")
    print(f"Transformers: {'âœ“' if transformers_ok else 'âŒ'}")
    print(f"PEFT: {'âœ“' if peft_ok else 'âŒ'}")
    print(f"Datasets: {'âœ“' if datasets_ok else 'âŒ'}")
    
    if torch_ok and transformers_ok and peft_ok and datasets_ok:
        print("\nğŸ‰ ç¯å¢ƒé…ç½®å®Œæ•´ï¼Œå¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼")
    else:
        print("\nâš ï¸  ç¯å¢ƒé…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ç¼ºå¤±çš„ç»„ä»¶ã€‚")
        
        if not torch_ok:
            print("  - éœ€è¦å®‰è£…æ”¯æŒ CUDA çš„ PyTorch")
        if not transformers_ok:
            print("  - éœ€è¦å®‰è£… Transformers")
        if not peft_ok:
            print("  - éœ€è¦å®‰è£… PEFT")
        if not datasets_ok:
            print("  - éœ€è¦å®‰è£… Datasets")


def main():
    """ä¸»å‡½æ•°"""
    print("CUDA ç¯å¢ƒæµ‹è¯•è„šæœ¬")
    print("=" * 50)
    
    all_results = {}
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    all_results['python'] = test_python_version()
    all_results['torch'] = test_torch_cuda()
    all_results['transformers'] = test_transformers()
    all_results['peft'] = test_peft()
    all_results['bitsandbytes'] = test_bitsandbytes()
    all_results['accelerate'] = test_accelerate()
    all_results['datasets'] = test_datasets()
    
    # ç”Ÿæˆæ€»ç»“
    generate_summary(all_results)
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 50)


if __name__ == "__main__":
    main() 
