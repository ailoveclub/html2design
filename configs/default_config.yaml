# Qwen3 训练框架默认配置

# 模型配置
model:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  model_path: null
  cache_dir: "./models"
  torch_dtype: "bfloat16"
  device_map: "auto"
  trust_remote_code: true
  use_flash_attention: true
  max_sequence_length: 2048

# 数据配置
data:
  train_file: "data/train.jsonl"
  val_file: "data/val.jsonl"
  test_file: "data/test.jsonl"
  input_column: "input"
  output_column: "output"
  max_input_length: 1024
  max_output_length: 1024
  data_format: "jsonl"
  preprocessing_num_workers: 4

# 训练配置
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: []
  run_name: null
  seed: 42
  fp16: false
  bf16: true
  gradient_checkpointing: true
  dataloader_pin_memory: true

# 推理配置
inference:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  repetition_penalty: 1.1
  pad_token_id: null
  eos_token_id: null

# 评估配置
evaluation:
  metrics: ["bleu", "rouge", "exact_match", "length"]
  batch_size: 8
  max_eval_samples: null

# LoRA配置
lora:
  use_lora: true
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  lora_dropout: 0.1
  bias: "none" 