# Qwen3 è®­ç»ƒæ¡†æ¶

ä¸€ä¸ªåŸºäº Hugging Face çš„ Qwen3 æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒæ•°æ®å‡†å¤‡ã€è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†å’Œéƒ¨ç½²ç­‰å®Œæ•´æµç¨‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **å®Œæ•´çš„è®­ç»ƒæµç¨‹**: æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†ã€éƒ¨ç½²
- ğŸ¯ **æ”¯æŒ LoRA å¾®è°ƒ**: é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•  
- âš™ï¸ **çµæ´»çš„é…ç½®ç³»ç»Ÿ**: æ”¯æŒ YAML/JSON é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
- ğŸ“Š **ä¸°å¯Œçš„è¯„ä¼°æŒ‡æ ‡**: BLEUã€ROUGEã€ç²¾ç¡®åŒ¹é…ç­‰å¤šç§è¯„ä¼°æŒ‡æ ‡
- ğŸŒ **ä¾¿æ·çš„éƒ¨ç½²é€‰é¡¹**: æ”¯æŒ Hugging Face Hubã€Docker ç­‰éƒ¨ç½²æ–¹å¼
- ğŸ’» **å‹å¥½çš„å‘½ä»¤è¡Œå·¥å…·**: æä¾›è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°ã€éƒ¨ç½²ç­‰å‘½ä»¤è¡Œå·¥å…·

## å®‰è£…

### ä»æºç å®‰è£…

```bash
git clone https://github.com/yourusername/qwen3-trainer.git
cd qwen3-trainer
pip install -e .
```

### ä¾èµ–è¦æ±‚

- Python >= 3.8
- PyTorch >= 2.0.0
- Transformers >= 4.35.0
- å…¶ä»–ä¾èµ–è§ `requirements.txt`

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

åˆ›å»ºè®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆJSONL æ ¼å¼ï¼‰ï¼š

```json
{"input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "output": "äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨ç ”ç©¶å¦‚ä½•è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å­¦ç§‘ã€‚"}
{"input": "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ", "output": "æœºå™¨å­¦ä¹ ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰ç§ç±»å‹ã€‚"}
```

### 2. è®­ç»ƒæ¨¡å‹

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒï¼š

```bash
qwen3-train --create-sample-data --use-lora
```

ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒï¼š

```bash
qwen3-train --train-file data/train.jsonl --val-file data/val.jsonl --use-lora
```

### 3. æ¨ç†æµ‹è¯•

äº¤äº’å¼æ¨ç†ï¼š

```bash
qwen3-infer --model-path ./outputs --is-lora --interactive
```

å•æ¬¡æ¨ç†ï¼š

```bash
qwen3-infer --model-path ./outputs --is-lora --prompt "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
```

å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```bash
qwen3-infer --model-path ./outputs --is-lora --start-server
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
qwen3-eval --model-path ./outputs --test-file data/test.jsonl --is-lora --generate-report
```

### 5. æ¨¡å‹éƒ¨ç½²

éƒ¨ç½²åˆ° Hugging Face Hubï¼š

```bash
qwen3-deploy --model-path ./outputs --action huggingface --repo-name yourusername/your-model
```

åˆ›å»º Docker éƒ¨ç½²åŒ…ï¼š

```bash
qwen3-deploy --model-path ./outputs --action docker --output-dir ./deployment
```

## è¯¦ç»†ä½¿ç”¨è¯´æ˜

### é…ç½®æ–‡ä»¶

æ¡†æ¶æ”¯æŒ YAML å’Œ JSON æ ¼å¼çš„é…ç½®æ–‡ä»¶ã€‚é»˜è®¤é…ç½®æ–‡ä»¶ä½äº `configs/default_config.yaml`ï¼š

```yaml
# æ¨¡å‹é…ç½®
model:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  torch_dtype: "bfloat16"
  use_flash_attention: true

# è®­ç»ƒé…ç½®  
training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  learning_rate: 5.0e-5
  
# LoRA é…ç½®
lora:
  use_lora: true
  r: 16
  lora_alpha: 32
```

### Python API ä½¿ç”¨

#### è®­ç»ƒæ¨¡å‹

```python
from qwen3_trainer import Qwen3Trainer
from qwen3_trainer.config import ConfigManager

# åŠ è½½é…ç½®
config = ConfigManager("configs/default_config.yaml")

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Qwen3Trainer(
    model_config=config.model_config,
    training_config=config.training_config,
    data_config=config.data_config,
    use_lora=True
)

# å¼€å§‹è®­ç»ƒ
trainer.run_full_training()
```

#### æ¨¡å‹æ¨ç†

```python
from qwen3_trainer import Qwen3Inference, ModelConfig, InferenceConfig

# é…ç½®
model_config = ModelConfig()
inference_config = InferenceConfig()

# åˆ›å»ºæ¨ç†å¼•æ“
inference = Qwen3Inference(
    model_config=model_config,
    inference_config=inference_config,
    model_path="./outputs",
    is_lora_model=True
)

# åŠ è½½æ¨¡å‹
inference.load_model()

# ç”Ÿæˆæ–‡æœ¬
response = inference.generate("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(response)
```

#### æ¨¡å‹è¯„ä¼°

```python
from qwen3_trainer import ModelEvaluator, EvaluationConfig

# é…ç½®è¯„ä¼°
eval_config = EvaluationConfig()
eval_config.metrics = ["bleu", "rouge", "exact_match"]

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = ModelEvaluator(eval_config)

# è¯„ä¼°æ¨¡å‹
results = evaluator.evaluate_model_on_dataset(
    model=inference,
    test_data=test_data
)

print(f"BLEU åˆ†æ•°: {results['bleu']:.4f}")
```

### æ•°æ®æ ¼å¼

æ¡†æ¶æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼š

#### JSONL æ ¼å¼ï¼ˆæ¨èï¼‰

```json
{"input": "è¾“å…¥æ–‡æœ¬1", "output": "è¾“å‡ºæ–‡æœ¬1"}
{"input": "è¾“å…¥æ–‡æœ¬2", "output": "è¾“å‡ºæ–‡æœ¬2"}
```

#### JSON æ ¼å¼

```json
[
  {"input": "è¾“å…¥æ–‡æœ¬1", "output": "è¾“å‡ºæ–‡æœ¬1"},
  {"input": "è¾“å…¥æ–‡æœ¬2", "output": "è¾“å‡ºæ–‡æœ¬2"}
]
```

#### CSV æ ¼å¼

```csv
input,output
è¾“å…¥æ–‡æœ¬1,è¾“å‡ºæ–‡æœ¬1
è¾“å…¥æ–‡æœ¬2,è¾“å‡ºæ–‡æœ¬2
```

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ¡†æ¶æ”¯æŒæ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼š

```python
from qwen3_trainer.evaluator import CustomMetrics

# HTML ç»“æ„ç›¸ä¼¼åº¦ï¼ˆé€‚ç”¨äº HTML è½¬æ¢ä»»åŠ¡ï¼‰
similarity = CustomMetrics.html_structure_similarity(pred_html, ref_html)

# JSON é”®è¦†ç›–ç‡ï¼ˆé€‚ç”¨äº JSON ç”Ÿæˆä»»åŠ¡ï¼‰
coverage = CustomMetrics.json_key_coverage(pred_json, ref_json)
```

## é€‚ç”¨åœºæ™¯

æœ¬æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š

1. **HTML è½¬ Figma JSON**: å°† HTML ä»£ç è½¬æ¢ä¸º Figma è®¾è®¡æ–‡ä»¶æ ¼å¼
2. **ä»£ç ç”Ÿæˆ**: æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç 
3. **æ–‡æœ¬æ‘˜è¦**: é•¿æ–‡æœ¬çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ
4. **å¯¹è¯ç³»ç»Ÿ**: æ„å»ºèŠå¤©æœºå™¨äººå’Œå¯¹è¯ AI
5. **å†…å®¹åˆ›ä½œ**: è‡ªåŠ¨åŒ–æ–‡ç« ã€æŠ¥å‘Šç­‰å†…å®¹ç”Ÿæˆ

## ç›®å½•ç»“æ„

```
qwen3-trainer/
â”œâ”€â”€ qwen3_trainer/          # æ ¸å¿ƒæ¡†æ¶ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ data.py             # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ trainer.py          # è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ inference.py        # æ¨ç†æ¨¡å—
â”‚   â”œâ”€â”€ evaluator.py        # è¯„ä¼°æ¨¡å—
â”‚   â””â”€â”€ deployment.py       # éƒ¨ç½²æ¨¡å—
â”œâ”€â”€ scripts/                # å‘½ä»¤è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ inference.py       # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py        # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ deploy.py          # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default_config.yaml
â”œâ”€â”€ examples/              # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ setup.py              # å®‰è£…è„šæœ¬
â””â”€â”€ README.md             # è¯´æ˜æ–‡æ¡£
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨ GPU**: è®­ç»ƒå’Œæ¨ç†æ—¶å»ºè®®ä½¿ç”¨ GPU åŠ é€Ÿ
2. **è°ƒæ•´æ‰¹å¤§å°**: æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ batch_size
3. **æ¢¯åº¦ç´¯ç§¯**: åœ¨æ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ gradient_accumulation_steps
4. **æ··åˆç²¾åº¦**: å¯ç”¨ bf16 æˆ– fp16 å‡å°‘æ˜¾å­˜ä½¿ç”¨
5. **LoRA å¾®è°ƒ**: å¯¹äºå¤§æ¨¡å‹æ¨èä½¿ç”¨ LoRA å¾®è°ƒ

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ˜¾å­˜ä¸è¶³**
   ```bash
   # å‡å°æ‰¹å¤§å°
   --batch-size 1
   # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   --gradient-checkpointing
   ```

2. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥æ¨¡å‹è·¯å¾„
   ls ./outputs
   # ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
   ```

3. **æ¨ç†é€Ÿåº¦æ…¢**
   ```bash
   # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ GPU
   nvidia-smi
   # å¯ç”¨ Flash Attention
   pip install flash-attn
   ```

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯: `git checkout -b feature/your-feature`
3. æäº¤æ›´æ”¹: `git commit -am 'Add some feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/your-feature`
5. æäº¤ Pull Request

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## æ›´æ–°æ—¥å¿—

### v0.1.0 (2024-01-xx)

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒ Qwen3 æ¨¡å‹å¾®è°ƒ
- å®ç°å®Œæ•´çš„è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°æµç¨‹
- æä¾›å‘½ä»¤è¡Œå·¥å…·å’Œ Python API
- æ”¯æŒ Hugging Face Hub å’Œ Docker éƒ¨ç½²

## è”ç³»æˆ‘ä»¬

- é¡¹ç›®ä¸»é¡µ: https://github.com/yourusername/qwen3-trainer
- é—®é¢˜åé¦ˆ: https://github.com/yourusername/qwen3-trainer/issues
- é‚®ç®±: your.email@example.com

## è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œç»„ç»‡ï¼š

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PEFT](https://github.com/huggingface/peft)
- [Qwen](https://github.com/QwenLM/Qwen)
- [PyTorch](https://pytorch.org/) 